---
title: 'Analyzing Results'
-- icon: 'chart-line'
iconType: 'regular'
head:
  - - meta
    - name: description
      content: Understand how to analyze A/B test results using statistical methods and drive data-driven decisions.

---

Once your A/B test is complete, the next crucial step is analyzing the results. The way you interpret these results will determine the effectiveness of your tests.

## Using Statistical Methods

### Statistical Significance
To determine if the difference between the baseline and the variation is meaningful, you need to check for **statistical significance**. A 95% confidence level is commonly used, meaning there's only a 5% chance the result happened by random chance.

### Key Metrics to Monitor:
- Duration: 14 days
- Confidence: 95%
- Sample size: 1000 visitors per variation (including the baseline)
- Success events: 50 success events per variation (including the baseline)

You can change these default metrics depending on your website traffic to get more accurate results. [Contact us](mailto:support@gleef.eu) for more information.

<Info>
    For more detailed guidance on statistical significance, refer to our [statistics Q&A.](/Q&A/statistics-data)
</Info>

## Interpreting Results

### Beyond Finding a Winner
Winning variations are not the only goal. Analyze why a variation performed better—was it the wording, the placement, or the clarity? Understanding the reasons behind success or failure will help you improve future tests.

## Making Data-Driven Decisions
Don’t just stop after identifying the winning variation. Use the insights gathered to continuously optimize your website. These learnings can inform new experiments and increase your overall effectiveness in improving user engagement and conversion rates.

<Tip>
    Keep testing new hypotheses and iterate based on your findings. <br/>
    Learn more about [data interpretation here.](/Q&A/statistics-data)
</Tip>