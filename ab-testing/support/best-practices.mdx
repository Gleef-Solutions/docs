---
title: 'Running A/B Tests'
head:
  - - meta
    - name: description
      content: Learn how to effectively run A/B tests with best practices, tips on creating variations, running experiments, avoiding bias, and analyzing results.

---

<Tip>Also read our [best practices](/best-practices)</Tip>

A/B testing wording can dramatically enhance user engagement and conversion rates. Tech-savvy companies routinely run thousands of A/B tests annually to optimize their user experience and conversions.

## Table of Contents
- [Creating Variations](#creating-variations)
- [Running Experiments](#running-experiments)
- [Advanced Experiments](#advanced-experiments)
- [Analyzing Results](#analyzing-results)
- [Best Practices FAQ](#best-practices-faq)

## Creating Variations

### Can I remove the default text?
No. Retain the default text as the baseline in your experiment. This acts as the control for evaluating the performance of other variations, ensuring accurate comparisons.

### How many variations should I experiment with?
Limit your variations to two or three to keep the experiment manageable and achieve statistical significance without requiring a massive sample size.

<Info>
    You'll need around 7,000 users per variation to detect a 1% change in performance relative to your baseline. Learn more about [statistical significance here.](/Q&A/statistics-data#statistical-significance)
</Info>

### What kind of wording should I experiment with?
Focus on high-impact text elements:
- **Headlines**
- **Call-to-Action (CTA) buttons**
- **Product descriptions**

Even small wording tweaks in these areas can drive significant improvements in user engagement and conversions.

### Why can't I run UI experiments with Gleef?
Gleef is focused on A/B testing text, not user interface elements. UI tests require different approaches and should be handled separately. Gleef aims to keep A/B testing easy, no-code, and focused on wording without altering your website's structure.

Our future roadmap includes assisting you in selecting the right words with even more precision.

---

## Running Experiments

### How long should I run my experiments?
Run your tests until you achieve statistical significance. However, we recommend not running an experiment for more than 2 months to avoid bias. For pages with sufficient traffic, you should reach significance within a few days. However, always run the experiment for at least 2 weeks to ensure reliable results.

<Tip>You can adjust these metrics by [contacting us](mailto:support@gleef.eu).</Tip>

### How do you ensure your experiments are not biased?
- Run the test for at least two weeks to account for external events or seasonal trends.
- Avoid making changes to the page while the test is active.
- Don’t run multiple overlapping experiments on the same page, as this could skew results. Each experiment should be isolated to a specific section of your website.

<Tip>Don't worry, Gleef also supports managing multiple experiments on the same page. Learn more [here](TK).</Tip>

### How many experiments can run at the same time?
You can run unlimited experiments across different pages, but avoid running multiple overlapping experiments on the same page. This ensures that the changes being tested are the only variables impacting user behavior.

<Tip>Gleef supports multiple experiments on a page, but it's best to avoid overlap. Learn how [here](TK).</Tip>

---

## Advanced Experiments

### What is multi-select, and when should I use it?
With **multi-select**, you can run experiments on multiple elements simultaneously to ensure consistency across your page. Use this feature when testing multiple similar elements (e.g., multiple CTA buttons).

### How do I define a click on a specific CTA as a success?
If you're testing an element that isn’t clickable, use a multi-select experiment. Select the element to experiment on, and add the CTA you want to measure clicks for. Define the variations for your main element and keep the CTA as the baseline.

---

## Analyzing Results

### What statistical methods should you use?
Use **confidence intervals** and **p-values** to analyze the results. A p-value less than 0.05 indicates statistical significance. Aim for a **95% confidence level** to ensure your results are reliable.

<Info>Learn more about statistical calculations in our [statistics Q&A.](/Q&A/statistics-data)</Info>

### How do you interpret the results?
It’s not just about finding the winning variation. Ask yourself:
- Why did a certain variation perform better?
- What elements of user behavior did it influence?

Use these insights to optimize future experiments and refine your overall website strategy.

---

## Best Practices FAQ

### How do I avoid stopping tests too early?
Stopping a test prematurely, even with promising early results, can lead to inconclusive data. Always run your tests for at least two weeks or until you achieve statistical significance.

### What should I avoid when running A/B tests?
- **Don’t change multiple elements at once:** It makes it harder to attribute performance differences to specific changes.
- **Ensure sufficient traffic:** Without enough visitors, your test may never reach statistical significance.
- **Avoid overlapping tests:** Running multiple tests on the same page can introduce bias.

### How can I make the most of failed tests?
Even if a variation doesn't outperform the baseline, it still provides valuable insights. Analyze what didn’t work and use that knowledge to guide future experiments.

### Should I retest after making website changes?
Yes. If you’ve made major updates to your site, retest key experiments to ensure the changes haven’t impacted their effectiveness.

---

By following these best practices and strategies, you can run more effective A/B tests and make data-driven decisions that improve your website’s performance and user engagement.